<!DOCTYPE 
html
>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Underlying Semantic Diffusion for Effective and Efficient In-Context Learning</title>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
    <style>
        /* CSS Document */
        body {
            font-family: 'Open Sans', sans-serif;
            font-weight: 300;
            background-color: #fff;
        }
        .content {
            width: 1000px;
            padding: 25px 50px;
            margin: 25px auto;
            background-color: white;
            box-shadow: 0px 0px 10px #999;
            border-radius: 15px;
        }
        a, a:visited {
            color: #224b8d;
            font-weight: 300;
            text-decoration: none;
        }
        h1 {
            text-align: center;
            font-size: 35px;
            font-weight: 300;
            margin-bottom: 30px;
        }
        h2 {
            font-size: 30px;
            font-weight: 300;
            text-align: center;
            margin: 40px 0;
        }
        p {
            line-height: 25px;
            text-align: justify;
            font-family: "Times New Roman", Times, serif;
            font-size: 18.8px;
            margin: 20px 0;
        }
        .figure-container {
            text-align: center;
            margin: 30px 0;
        }
        .figure {
            width: 100%;
            margin: 15px 0;
        }
        .caption {
            font-style: italic;
            font-size: 16px;
            margin-top: 10px;
        }
        .links {
            text-align: center;
            margin: 30px 0;
        }
        .links a {
            margin: 0 15px;
            font-weight: bold;
        }
        .bibtex-code {
            background-color: #f5f5f5;
            padding: 20px;
            border-radius: 5px;
            font-family: monospace;
            white-space: pre-wrap;
            margin: 20px 0;
        }

        video::-webkit-media-controls { display: none !important; }
        video { width: 100%; margin: 20px auto; }
    </style>
</head>
<body>
    <div class="content">
        <h1>Underlying Semantic Diffusion for Effective and Efficient In-Context Learning</h1>
        <div style="text-align: center; margin-bottom: 30px; font-size: 20px;">
            Zhong Ji<sup>1</sup>, Weilong Cao<sup>1</sup>, Yan Zhang<sup>1</sup>, Yanwei Pang<sup>1</sup>, Jungong Han, Xuelong Li<br>
            <sup>1</sup>Tianjin University, Tianjin, China
        </div>
        <div class="links">
            <a href="https://arxiv.org/abs/2503.04050" target="_blank">[Paper]</a>
            <a href="https://github.com/dragon-cao/US-Diffusion" target="_blank">[Code]</a>
        </div>
    </div>

    <div class="content">
        <h2>Abstract</h2>
        <p>
            Diffusion models has emerged as a powerful framework for tasks like image controllable generation and dense prediction. However, existing models often struggle to capture underlying semantics (e.g., edges, textures, shapes) and effectively utilize in-context learning, limiting their contextual understanding and image generation quality. Additionally, high computational costs and slow inference speeds hinder their real-time applicability. To address these challenges, we propose Underlying Semantic Diffusion (US-Diffusion), an enhanced diffusion model that boosts underlying semantics learning, computational efficiency, and in-context learning capabilities on multi-task scenarios. We introduce Separate \& Gather Adapter (SGA), which decouples input conditions for different tasks while sharing the architecture, enabling better in-context learning and generalization across diverse visual domains. We also present a Feedback-Aided Learning (FAL) framework, which leverages feedback signals to guide the model in capturing semantic details and dynamically adapting to task-specific contextual cues. Furthermore, we propose a plug-and-play Efficient Sampling Strategy (ESS) for dense sampling at time steps with high-noise levels, which aims at optimizing training and inference efficiency while maintaining strong in-context learning performance. Experimental results demonstrate that US-Diffusion outperforms the state-of-the-art method, achieving an average reduction of 7.47 in FID on Map2Image tasks and an average reduction of 0.026 in RMSE on Image2Map tasks, while achieving approximately 9.45× faster inference speed. Our method also demonstrates superior training efficiency and in-context learning capabilities, excelling in new datasets and tasks, highlighting its robustness and adaptability across diverse visual domains.
        </p>
        <div class="figure-container">
            <img src="./assets/motivation.png" class="figure">
            </div>
        </div>
    </div>

    <div class="content">
        <h2>Framework</h2>
        <p>
            The proposed framework, US-Diffusion, aims to overcome the limitations of current diffusion models in terms of generation quality and computational efficiency. As illustrated in this figure, the framework comprises four main components: the widely-used Stable Diffusion (SD) model, the ControlNet module equipped with our Separate & Gather Adapter (SGA), the proposed Feedback-Aided Learning (FAL) framework, and the conditional input component. Together, these components enhance the model's ability to better capture underlying semantic information and improve generation quality. To tackle the issue of slow inference speed that is typical in diffusion models, we also propose an innovative Efficient Sampling Strategy (ESS) employed during the noise-adding and denoising stages.
        </p>
        <div class="figure-container">
            <img src="./assets/framework.png" class="figure">
        </div>
    </div>

    <div class="content">
        <video id="demoVideo" 
               autoplay  <!-- 添加此属性 -->
               muted
               playsinline
               loop
               preload="auto"  <!-- 添加预加载 -->
               poster="./assets/demo_preview.jpg">
               style="width:100%;height:100%">
        <source src="/assets/demo_video.mp4" type="video/mp4">
        <source src="/assets/demo_video.webm" type="video/webm">
        </video>
        
        <script>
            / 修改后的播放控制逻辑
            document.addEventListener('DOMContentLoaded', function() {
                const video = document.getElementById('demoVideo');
                let interactionOccurred = false;
            
                // 通过任意页面交互触发播放
                document.addEventListener('click', function() {
                    if (!interactionOccurred) {
                        interactionOccurred = true;
                        video.play().catch(e => console.error('播放失败:', e));
                    }
                });
            
                // 自动播放尝试
                const playPromise = video.play();
                if (playPromise !== undefined) {
                    playPromise.catch(() => {
                        console.log('自动播放被阻止，等待用户交互...');
                    });
                }
            });
        </script>
    </div>

    <div class="content">
        <h2>Experimental Results</h2>
        <div class="figure-container">
            <img src="./assets/result1.png" class="figure">
            <div class="caption">Map2Image tasks</div>
        </div>
        <div class="figure-container">
            <img src="./assets/result2.png" class="figure">
            <div class="caption">Image2Map tasks</div>
        </div>
    </div>

    <div class="content">
        <h2>BibTeX</h2>
        <pre class="bibtex-code">@article{ji2025underlying,
  title={Underlying Semantic Diffusion for Effective and Efficient In-Context Learning},
  author={Ji, Zhong and Cao, Weilong and Zhang, Yan and Pang, Yanwei and Han, Jungong and Li, Xuelong},
  journal={arXiv preprint arXiv:2503.04050},
  year={2025}
}
</pre>
    </div>
</body>
</html>
